import pandas as pd
#from dataprep.eda import create_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Load the dataset (UCI Adult Income dataset)
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'
columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 
           'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 
           'hours_per_week', 'native_country', 'income']

data = pd.read_csv(url, names=columns, na_values=' ?', sep=',\s', engine='python')

# EDA Report using Dataprep
#create_report(data).show_browser()

# Drop missing values
data_clean = data.dropna()

# Features and target variable
X = data_clean.drop('income', axis=1)
y = data_clean['income'].apply(lambda x: 1 if x == '>50K' else 0)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define numeric and categorical columns
#numeric_features = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']
#categorical_features = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']
numeric_features = ['age', 'fnlwgt', 'education_num','hours_per_week']
categorical_features = ['workclass', 'education', 'occupation', 'race', 'sex', 'native_country']

# Preprocessing pipelines for numeric and categorical features
# This pipeline processes numeric features. It consists of two steps:
# Imputation is used to handle missing values in numeric data. Here, the missing values are replaced by the median of the corresponding feature.
# Using the median is a robust approach as it is less sensitive to outliers compared to the mean.
# Standardization scales the numeric features so that they have a mean of 0 and a standard deviation of 1. This is crucial for many machine learning models 
# Scaling ensures that all numeric features are on a similar scale, improving the performance of models


numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())])

# This pipeline processes categorical features. It also consists of two steps:
# This imputes missing values in categorical features by filling them with the most frequent (mode) value of the feature. This is useful when some values are missing, but the feature has a dominant category
# One-Hot Encoding converts categorical variables into a format suitable for machine learning models. It creates binary columns for each category in the feature

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])

# Apply the transformers
# ColumnTransformer allows you to apply different preprocessing pipelines to different sets of features in your dataset. In this case, it applies the numeric_transformer to the numeric features and the categorical_transformer to the categorical features.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

# Preprocess train and test sets
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)
print('-----------Preprocessing completed-------------')
# AutoML steps using TPOT. This will run on our data and produce the best fitting model recommendation.
print('-----------Starting AutoML-------------')
from tpot import TPOTClassifier
from sklearn.metrics import classification_report

# TPOT AutoML classifier (optimize for 5 generations - using genetic algorithm)
#generations=5: The number of optimization cycles (generations) to run.
#population_size=50: The number of pipelines to explore in each generation.
#verbosity=2: Print detailed logs during execution.
#random_state=42: Set a seed for reproducibility of results.
tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)
tpot.fit(X_train_preprocessed, y_train)

# Evaluate the best model on the test set
y_pred = tpot.predict(X_test_preprocessed)
print(classification_report(y_test, y_pred))

# Export the best pipeline generated by TPOT
tpot.export('best_pipeline_5gen.py')
